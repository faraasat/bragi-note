# Bragi Note - Final Architecture (MVP)

## ‚úÖ Technology Stack

### Core Technologies
- **Frontend**: Next.js 16 (React 19, App Router, Tailwind CSS v4)
- **AI Provider**: Groq API (Llama 3.3 70B - Free tier: 7,000 RPM, 200M+ tokens/day)
- **Workflow Orchestration**: Kestra (@kestra-io/libs v1.1.0)
- **Deployment**: Vercel (Serverless functions)
- **Monorepo**: Turborepo + Yarn workspaces

### Package Structure
```
packages/
‚îî‚îÄ‚îÄ ai/
    ‚îú‚îÄ‚îÄ groq-sdk          # Direct AI provider (Llama 3.3 70B)
    ‚îî‚îÄ‚îÄ @kestra-io/libs   # Workflow orchestration with logging & metrics
```

## üèóÔ∏è Architecture Overview

### Data Flow
```
User Request (Browser)
    ‚Üì
Next.js Frontend (Dashboard)
    ‚Üì
API Routes (/api/ai/catch-up, /rewrite, /explain)
    ‚Üì
Kestra Workflow Orchestration
    ‚îú‚îÄ Structured logging (logger.info, logger.error)
    ‚îú‚îÄ Metrics tracking (counter, timer, gauge)
    ‚îú‚îÄ Step-by-step execution tracking
    ‚îî‚îÄ Output management
    ‚Üì
Groq API Client (groq-sdk)
    ‚Üì
Groq Cloud (Llama 3.3 70B Model)
    ‚Üì
Response flows back through all layers
```

### Why This Architecture?

#### ‚úÖ Groq (No Oumi Framework)
- **Direct Integration**: Using `groq-sdk` npm package directly
- **Reasons**:
  - Oumi is primarily Python-based (for model training/fine-tuning)
  - Not needed for inference-only use case
  - Groq SDK is simpler and more direct
  - Free tier is generous (7,000 RPM)
- **Model**: Llama 3.3 70B Versatile (fast, accurate, free)

#### ‚úÖ Kestra (@kestra-io/libs)
- **Official Package**: Using Kestra's JavaScript/TypeScript library
- **Features Used**:
  - **Structured Logging**: `logger.info()`, `logger.error()`, `logger.warn()`
  - **Metrics Tracking**: `counter()` for success/failure rates
  - **Performance Monitoring**: `timer()` for duration tracking
  - **Gauges**: `gauge()` for urgency levels and other measurements
  - **Outputs**: `outputs()` for workflow result metadata
- **Vercel Compatible**: Works as regular Node.js code (no separate server needed)
- **Graceful Degradation**: Functions fail silently if not in Kestra environment

## üì¶ Dependencies

### Production Dependencies
```json
{
  "@kestra-io/libs": "^1.1.0",    // Workflow orchestration
  "groq-sdk": "^0.7.0",            // Groq AI API client
  "@ai-sdk/openai": "^0.0.66",    // Vercel AI SDK (future use)
  "ai": "^3.4.33",                 // Vercel AI SDK (future use)
  "axios": "^1.13.2",              // HTTP client (Kestra dep)
  "zod": "^3.23.8"                 // Type validation
}
```

### Why @kestra-io/libs is Mandatory

1. **Professional Workflow Management**
   - Structured multi-step pipeline execution
   - Automatic step tracking and timing
   - Error handling with detailed logging

2. **Production Monitoring**
   - Metrics for success/failure rates
   - Performance timers for each workflow step
   - Gauges for business metrics (urgency levels, etc.)

3. **Observability**
   - Structured logs compatible with log aggregation tools
   - Output metadata for downstream processing
   - Ready for Kestra server integration (Phase 2)

4. **Future-Proof**
   - Can migrate to full Kestra server later
   - Workflow definitions easily portable to YAML
   - Same codebase works standalone or with Kestra platform

## üîÑ Kestra Integration Details

### Current Implementation (Serverless)
```typescript
import Kestra from "@kestra-io/libs";

// Initialize
const logger = Kestra.logger();

// In workflow execution:
logger.info("Starting catch-up workflow", { executionId, userId });

// Track metrics
Kestra.counter("workflow.catchup.started", 1, { userId });

// Time operations
const aiTimer = Kestra.timer("workflow.catchup.ai_analysis_duration");
const result = await groqClient.generate(...);
aiTimer(duration);

// Output results
Kestra.outputs({
  executionId,
  duration,
  decisionsCount: result.decisions.length
});
```

### Workflow Steps (All 3 Features)

#### 1. Catch-Up Workflow
```
Step 1: validate-input    ‚Üí Check text length, format
Step 2: preprocess-content ‚Üí Clean whitespace, normalize
Step 3: ai-analysis       ‚Üí Call Groq API (timed)
Step 4: format-results    ‚Üí Structure response
```

#### 2. Rewrite Workflow
```
Step 1: validate-input    ‚Üí Check message validity
Step 2: analyze-tone      ‚Üí Detect angry/urgent words
Step 3: ai-rewrite        ‚Üí Call Groq API (timed)
Step 4: quality-check     ‚Üí Verify output differs from input
```

#### 3. Explain Workflow
```
Step 1: validate-input    ‚Üí Check content length
Step 2: detect-category   ‚Üí Auto-detect medical/legal/financial/technical
Step 3: ai-explanation    ‚Üí Call Groq API (timed)
Step 4: add-disclaimers   ‚Üí Add category-specific disclaimers
```

## üöÄ Vercel Deployment

### Compatibility
‚úÖ **Fully Vercel-Compatible**
- Kestra libs work as regular Node.js functions
- No separate services or servers required
- All metrics/logging work in serverless environment
- Functions complete within 60s timeout

### Environment Variables
```bash
# Required
GROQ_API_KEY=gsk_...

# Optional
NEXT_PUBLIC_APP_URL=https://your-domain.vercel.app
```

### Deployment Command
```bash
vercel --prod
```

## üìä Metrics & Monitoring

### Kestra Metrics Available

#### Counters (Track occurrences)
- `workflow.catchup.started` - Workflow initiation count
- `workflow.catchup.completed` - Successful completions
- `workflow.catchup.failed` - Failed executions
- `workflow.rewrite.started` / `.completed` / `.failed`
- `workflow.explain.started` / `.completed` / `.failed`

#### Timers (Track duration)
- `workflow.catchup.ai_analysis_duration` - Time spent in AI call
- `workflow.catchup.total_duration` - End-to-end workflow time
- Similar for rewrite and explain workflows

#### Gauges (Track levels)
- `workflow.explain.urgency_level` - Document urgency (1=low, 2=medium, 3=high)

### Logging Levels
- `logger.info()` - Normal workflow progress
- `logger.error()` - Failures and exceptions
- `logger.warn()` - Warnings and edge cases

## üéØ Current Status

### ‚úÖ What's Working
1. **All 3 AI Features**: Catch-up, Rewrite, Explain
2. **Kestra Integration**: Logging, metrics, timers, outputs
3. **Groq API**: Llama 3.3 70B model (sub-second responses)
4. **Vercel Ready**: Builds successfully, serverless-compatible
5. **Health Checks**: `/api/ai/health` endpoint functional

### üìà Performance
- **Catch-Up**: ~600ms average
- **Rewrite**: ~540ms average
- **Explain**: ~475ms average
- **Health Check**: ~100ms

### üí∞ Cost Breakdown
| Service | Usage | Cost |
|---------|-------|------|
| Groq API | 7,000 RPM, 200M+ tokens/day | $0 |
| Kestra libs | Unlimited (npm package) | $0 |
| Vercel | 100 GB-hours/month | $0 |
| **Total** | | **$0/month** |

## üîÆ Future Enhancements (Phase 2)

### Option 1: Self-Hosted Kestra Server
- Deploy Kestra server separately
- Visual workflow designer
- Advanced scheduling and retry logic
- Workflow history and analytics dashboard
- **Cost**: $5-10/month (small VPS)

### Option 2: Keep Serverless (Current)
- Maintain current architecture
- Scale with Vercel's infrastructure
- No additional infrastructure management
- **Cost**: $0 (free tier sufficient for MVP)

## üìù Key Decisions Made

1. **No Oumi Framework**: Using Groq SDK directly (simpler, more direct)
2. **Kestra as Library**: Using `@kestra-io/libs` not separate server (Vercel-compatible)
3. **Model Choice**: Llama 3.3 70B (updated from decommissioned 3.1)
4. **Free Tier Only**: All services on free tiers for MVP

## üõ†Ô∏è Development Commands

```bash
# Install dependencies
yarn install

# Run development server
yarn workspace @bragi/web dev

# Build for production
yarn build

# Type check
yarn workspace @bragi/ai type-check

# Test health endpoint
curl http://localhost:3000/api/ai/health

# Test catch-up feature
curl -X POST http://localhost:3000/api/ai/catch-up \
  -H "Content-Type: application/json" \
  -d '{"text":"Meeting notes here","userId":"test"}'
```

## üìö Documentation Files

- [ARCHITECTURE.md](./ARCHITECTURE.md) - Detailed architecture diagrams
- [VERCEL_DEPLOYMENT.md](./VERCEL_DEPLOYMENT.md) - Deployment guide
- [OUMI_KESTRA_GUIDE.md](./OUMI_KESTRA_GUIDE.md) - Implementation details
- [ENV_SETUP.md](./ENV_SETUP.md) - Environment configuration

## ‚ú® Summary

**Final Tech Stack**:
- ‚úÖ **Groq** (Direct SDK, no Oumi framework)
- ‚úÖ **Kestra** (@kestra-io/libs for workflows, logging, metrics)
- ‚úÖ **Vercel** (Serverless deployment)
- ‚úÖ **Next.js 16** (Frontend + API routes)

**Why This Works**:
- Simple, maintainable, free
- Production-ready monitoring with Kestra
- Scales with Vercel
- Can upgrade to full Kestra server later if needed

---

**Status**: ‚úÖ Ready for MVP deployment on Vercel
**Last Updated**: December 14, 2025
